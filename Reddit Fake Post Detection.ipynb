{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "competition_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1IcdS3c9P_ZM",
        "Dpk7pMW-QFcy",
        "9tgH-dyWQUYz",
        "7AYPyzNWUwqc",
        "450R0-9xU621",
        "82V5fSgKVO5F",
        "r6MWWP77Vl9G",
        "GdxrewLgmJYy",
        "rrgSBmphWBGs",
        "cYZcrA5nWFIk",
        "5IF1f6L1WILt",
        "bX2yXQIdWdJ8",
        "7NANJPrdA_jT",
        "HPm1QLTJC6zk",
        "QMSrBgO8jf_K",
        "LFiHuCDoiedG",
        "H9t43e1VEZD3",
        "TOPNSrk5W8yA",
        "HfKxkMy1Vrf0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import main libraries"
      ],
      "metadata": {
        "id": "1IcdS3c9P_ZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W1c_BRHjJPSE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import nltk \n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "Dpk7pMW-QFcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "read our train and test data using pd.read_csv function"
      ],
      "metadata": {
        "id": "g_TykzulEwlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('xy_train.csv',index_col='id') # read train data (xy_train.csv) file\n",
        "test_data=pd.read_csv('x_test.csv',index_col='id')    # read test data (x_test.csv) file"
      ],
      "metadata": {
        "id": "5dYNIFUkJUYw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print train data\n",
        "train_data "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "9Pi7FpdxJW7_",
        "outputId": "dd3c46e2-ee16-4e3b-df0f-4ed4f4de52dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  label\n",
              "id                                                              \n",
              "265723  A group of friends began to volunteer at a hom...    0.0\n",
              "284269  British Prime Minister @Theresa_May on Nerve A...    0.0\n",
              "207715  In 1961, Goodyear released a kit that allows P...    0.0\n",
              "551106  Happy Birthday, Bob Barker! The Price Is Right...    0.0\n",
              "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...    0.0\n",
              "...                                                   ...    ...\n",
              "244580  Convention Crowd Really Hoping Bill Clinton Br...    0.0\n",
              "309472  North Korean officials playing in a band, down...    1.0\n",
              "202030  Deleted scene from Star Wars where the rebel a...    0.0\n",
              "503084  I bought a grinder at a government-run cannabi...    1.0\n",
              "62769   Billionaire Richard Branson offers Makepeace I...    NaN\n",
              "\n",
              "[40949 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f7b538a-0abd-465c-bb4c-366a7bc45d09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265723</th>\n",
              "      <td>A group of friends began to volunteer at a hom...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284269</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207715</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551106</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8584</th>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244580</th>\n",
              "      <td>Convention Crowd Really Hoping Bill Clinton Br...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309472</th>\n",
              "      <td>North Korean officials playing in a band, down...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202030</th>\n",
              "      <td>Deleted scene from Star Wars where the rebel a...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503084</th>\n",
              "      <td>I bought a grinder at a government-run cannabi...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62769</th>\n",
              "      <td>Billionaire Richard Branson offers Makepeace I...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40949 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f7b538a-0abd-465c-bb4c-366a7bc45d09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f7b538a-0abd-465c-bb4c-366a7bc45d09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f7b538a-0abd-465c-bb4c-366a7bc45d09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I found  that our text data includes noise in form of punctuation, text in different cases, special characters, url and numbers So we need to clean this data to get good modelling results"
      ],
      "metadata": {
        "id": "8ZMjtrhkQKgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning and preprocessing text data\n"
      ],
      "metadata": {
        "id": "9tgH-dyWQUYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What preprocessing steps are used?**"
      ],
      "metadata": {
        "id": "iH6xd8-C3M_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For good modelling results, we need consistent and clean data. No matter how advanced your model is, the basic principle remains the same: trash in, trash out. As a result, data preprocessing is an important stage in developing a Machine Learning model, and the outcomes are dependent on how effectively the data has been preprocessed.\n",
        "\n",
        "Text preprocessing is the initial stage in the NLP model construction process.\n",
        "\n",
        "\n",
        "Preprocessing steps:\n",
        "\n",
        "* Removing the URL \n",
        "\n",
        "* Removing all non-essential letters (Numbers and Punctuation)\n",
        "\n",
        "* Convert all characters to lowercase.\n",
        "\n",
        "* Tokenization.\n",
        "\n",
        "* Removing stopwords\n",
        "\n",
        "* Lemmatization\n",
        "\n",
        "* Stemming \n",
        "\n",
        "* Remove the words with a length of 2 \n",
        "\n",
        "* Return the list of tokens to the string."
      ],
      "metadata": {
        "id": "ZdQl-xUXWCl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**URLs** references to a location on the web, but do not provide any additional information.\n",
        "\n",
        "So it's better to  remove these using the library named re\n",
        "We take our sample text and analyse each word, removing words or strings starting with http."
      ],
      "metadata": {
        "id": "sG5XM_dUSW_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function removing Url from text data \n",
        "def remove_urls(_text):\n",
        "    return re.sub(r'http\\S+','',_text)\n",
        "train_data['clean_text']=train_data['text'].apply(remove_urls) # apply function on train data\n",
        "test_data['clean_text']=test_data['text'].apply(remove_urls)   # apply function on test data"
      ],
      "metadata": {
        "id": "exr1FvzzJoR_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Remove any numbers (0–9) that aren't important to our analysis.\n",
        "\n",
        "Punctuation will also be removed. Punctuation is a collection of symbols. [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]"
      ],
      "metadata": {
        "id": "p--5JMztS-WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function removing all irrelevant characters ( special characters, numbers and punctuation)\n",
        "\n",
        "def remove_non_alphanumeric(_text):\n",
        "    return re.sub('[^a-zA-Z]',' ',_text)\n",
        "train_data['clean_text']=train_data['clean_text'].apply(remove_non_alphanumeric) # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(remove_non_alphanumeric)   # apply function on test data"
      ],
      "metadata": {
        "id": "gRQ1EFZDJsRv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid repetition, all words are converted to lower. Because if this step is skipped  \"Phone\" and \"phone\" will be treated as two independent words."
      ],
      "metadata": {
        "id": "oPisZ7eoTbcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function converting all uppercase to lowercase\n",
        "def to_lowercase(_text):\n",
        "    return str(_text).lower()\n",
        "train_data['clean_text']=train_data['clean_text'].apply(to_lowercase) # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(to_lowercase)  # apply function on test data"
      ],
      "metadata": {
        "id": "Oum7AzUlJui_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is the process of breaking down a text into smaller pieces known as tokens. Tokens can include words, numbers, punctuation marks, and other symbols.\n",
        "\n",
        "I use it here to convert sentence into words to make it easier to remove stop words"
      ],
      "metadata": {
        "id": "QuQZv5XjTge-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "# function to  tokenaze data \n",
        "def _tokenization(_text):\n",
        "   return word_tokenize(_text)\n",
        "train_data['clean_text']=train_data['clean_text'].apply(_tokenization) # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(_tokenization)   # apply function on test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCU1CNNKJwVv",
        "outputId": "00996336-a42a-4975-f3a8-1af1f28a82b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords** are the most commonly used words in a language, such as \"an\" , \"a\" , \"our\" ,\"are\", \"into\" , and \"they.\" These words have little meaning and are frequently removed from text. So it's better to remove them"
      ],
      "metadata": {
        "id": "GkrQXJm-TupT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "# function removing stop words\n",
        "def remove_stopwords(token):\n",
        "    return[item for item in token if item not in stop_words]    # return words which are not stop words \n",
        "train_data['clean_text']=train_data['clean_text'].apply(remove_stopwords) # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(remove_stopwords)   # apply function on test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7UgrP5VJyGf",
        "outputId": "cd2a995c-12a5-460f-8efe-097787a47f92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**lemmatization** consists in doing things properly with the use of a vocabulary and morphological analysis of words, to return the base or dictionary form of a word, which is known as the lemma.[6]\n",
        "\n"
      ],
      "metadata": {
        "id": "WEblK2BBUMdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "lemma=WordNetLemmatizer()\n",
        "\n",
        "def lemmatization(token):\n",
        "    return [lemma.lemmatize(word=w,pos='v') for w in token]\n",
        "train_data['clean_text']=train_data['clean_text'].apply(lemmatization)\n",
        "test_data['clean_text']=test_data['clean_text'].apply(lemmatization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncx_KBddJ0RP",
        "outputId": "13745138-0c47-49aa-d7a7-957d3373ed39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is a natural language processing technique that helps with text preprocessing by lowering inflection in words to their base forms."
      ],
      "metadata": {
        "id": "ZTWxlCHDUi4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "# stemming function \n",
        "def stem(token):\n",
        "    return[stemmer.stem(i) for i in token] # return  a common base or root of word \n",
        "train_data['clean_text']=train_data['clean_text'].apply(stem) # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(stem)   # apply function on test data"
      ],
      "metadata": {
        "id": "n3gkk_7TJ2Pv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is some kind of noise in our text after executing all required text processing processes, so I am removing the words that are really short in length."
      ],
      "metadata": {
        "id": "wCP9J7GTUnTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function removeing the words having length <= 2\n",
        "def length(token):\n",
        "    return [i for i in token if len(i) > 2] # return words which having length more than 2 characters \n",
        "train_data['clean_text']=train_data['clean_text'].apply(length) # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(length)  # apply function on test data"
      ],
      "metadata": {
        "id": "q5zgYYbdJ4RA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['clean_text'] # text in a form of list, so need to convert it into string form "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwTfmdZKJ6fg",
        "outputId": "df89eb4b-63fa-4426-ed66-1854c6655a16"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "265723    [group, friend, begin, volunt, homeless, shelt...\n",
              "284269    [british, prime, minist, theresa, may, nerv, a...\n",
              "207715    [goodyear, releas, kit, allow, bring, heel, zw...\n",
              "551106    [happi, birthday, bob, barker, price, right, h...\n",
              "8584      [obama, nation, innoc, cop, unarm, young, blac...\n",
              "                                ...                        \n",
              "244580    [convent, crowd, realli, hop, bill, clinton, b...\n",
              "309472    [north, korean, offici, play, band, downtown, ...\n",
              "202030    [delet, scene, star, war, rebel, allianc, atta...\n",
              "503084    [buy, grinder, govern, run, cannabi, retail, c...\n",
              "62769     [billionair, richard, branson, offer, makepeac...\n",
              "Name: clean_text, Length: 40949, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function converting a list of tokens back into a string.\n",
        "def convert_to_string(list_text):\n",
        "    return ' '.join(list_text) # return a string of our text \n",
        "train_data['clean_text']=train_data['clean_text'].apply(convert_to_string)   # apply function on train data\n",
        "test_data['clean_text']=test_data['clean_text'].apply(convert_to_string)     # apply function on test data"
      ],
      "metadata": {
        "id": "pyZhSc_VJ8ig"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['clean_text'] # after applying  (convert_to_string ) function to convert into a string form"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMHVsu21J-6Q",
        "outputId": "e95f55a2-442e-46cb-bb43-3889d6d198a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "265723    group friend begin volunt homeless shelter nei...\n",
              "284269    british prime minist theresa may nerv attack f...\n",
              "207715    goodyear releas kit allow bring heel zwillc fi...\n",
              "551106    happi birthday bob barker price right host lik...\n",
              "8584      obama nation innoc cop unarm young black men d...\n",
              "                                ...                        \n",
              "244580    convent crowd realli hop bill clinton break te...\n",
              "309472    north korean offici play band downtown pyongya...\n",
              "202030    delet scene star war rebel allianc attack impe...\n",
              "503084    buy grinder govern run cannabi retail come wra...\n",
              "62769     billionair richard branson offer makepeac isla...\n",
              "Name: clean_text, Length: 40949, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(train_data) # print train data to show 'text' after apply cleaning and pre processing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "Odf4I0a5KFIP",
        "outputId": "aba1bf3a-f888-4012-e558-ad031c80bb70"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  label  \\\n",
              "id                                                                 \n",
              "265723  A group of friends began to volunteer at a hom...    0.0   \n",
              "284269  British Prime Minister @Theresa_May on Nerve A...    0.0   \n",
              "207715  In 1961, Goodyear released a kit that allows P...    0.0   \n",
              "551106  Happy Birthday, Bob Barker! The Price Is Right...    0.0   \n",
              "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...    0.0   \n",
              "...                                                   ...    ...   \n",
              "244580  Convention Crowd Really Hoping Bill Clinton Br...    0.0   \n",
              "309472  North Korean officials playing in a band, down...    1.0   \n",
              "202030  Deleted scene from Star Wars where the rebel a...    0.0   \n",
              "503084  I bought a grinder at a government-run cannabi...    1.0   \n",
              "62769   Billionaire Richard Branson offers Makepeace I...    NaN   \n",
              "\n",
              "                                               clean_text  \n",
              "id                                                         \n",
              "265723  group friend begin volunt homeless shelter nei...  \n",
              "284269  british prime minist theresa may nerv attack f...  \n",
              "207715  goodyear releas kit allow bring heel zwillc fi...  \n",
              "551106  happi birthday bob barker price right host lik...  \n",
              "8584    obama nation innoc cop unarm young black men d...  \n",
              "...                                                   ...  \n",
              "244580  convent crowd realli hop bill clinton break te...  \n",
              "309472  north korean offici play band downtown pyongya...  \n",
              "202030  delet scene star war rebel allianc attack impe...  \n",
              "503084  buy grinder govern run cannabi retail come wra...  \n",
              "62769   billionair richard branson offer makepeac isla...  \n",
              "\n",
              "[40949 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1173cca-9f57-4178-8817-7f6ca7898b1d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265723</th>\n",
              "      <td>A group of friends began to volunteer at a hom...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>group friend begin volunt homeless shelter nei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284269</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>british prime minist theresa may nerv attack f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207715</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>goodyear releas kit allow bring heel zwillc fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551106</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>happi birthday bob barker price right host lik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8584</th>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>obama nation innoc cop unarm young black men d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244580</th>\n",
              "      <td>Convention Crowd Really Hoping Bill Clinton Br...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>convent crowd realli hop bill clinton break te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309472</th>\n",
              "      <td>North Korean officials playing in a band, down...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>north korean offici play band downtown pyongya...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202030</th>\n",
              "      <td>Deleted scene from Star Wars where the rebel a...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>delet scene star war rebel allianc attack impe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503084</th>\n",
              "      <td>I bought a grinder at a government-run cannabi...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>buy grinder govern run cannabi retail come wra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62769</th>\n",
              "      <td>Billionaire Richard Branson offers Makepeace I...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>billionair richard branson offer makepeac isla...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40949 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1173cca-9f57-4178-8817-7f6ca7898b1d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1173cca-9f57-4178-8817-7f6ca7898b1d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1173cca-9f57-4178-8817-7f6ca7898b1d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop rows"
      ],
      "metadata": {
        "id": "7AYPyzNWUwqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here I dropped rows which it's value equals 2 \n",
        "becouse we in case of binary grade \n",
        "\n",
        "we want to predict if the tilte represent fake news (1) or not fake (0)"
      ],
      "metadata": {
        "id": "iHbI3wmzPnRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create binary grade, label  0-1  not fake or fake \n",
        "train_data.loc[train_data[\"label\"] > 1] = np.NaN\n",
        "# Drop when any of x missing\n",
        "train_data = train_data[(train_data[\"clean_text\"] != \"\") & (train_data[\"clean_text\"] != \"null\")]\n",
        "train_data = train_data.dropna(axis=\"index\", subset=[\"label\", \"text\", \"clean_text\"]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3MWupvsiKG7g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive data \n",
        "\n"
      ],
      "metadata": {
        "id": "450R0-9xU621"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show most common  & uncommon words\n",
        "\n",
        "Even if we are dealing with text, descriptive analysis should be used to have a better understanding of the data.\n"
      ],
      "metadata": {
        "id": "HGt1i7kkPy17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word Frequency of most common words\n",
        "word_freq = pd.Series(\" \".join(train_data[\"clean_text\"]).split()).value_counts()\n",
        "pd.DataFrame(word_freq[10:20].reset_index(name=\"freq\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "vLzqxPwRKJpx",
        "outputId": "51ff4aaf-c9f3-4ae3-df46-6b52c96f1f35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    index  freq\n",
              "0   peopl  1879\n",
              "1   trump  1855\n",
              "2     man  1832\n",
              "3   color  1822\n",
              "4     use  1787\n",
              "5   first  1707\n",
              "6     old  1621\n",
              "7    time  1582\n",
              "8  poster  1562\n",
              "9     day  1503"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49f12eca-8e51-47c2-b9db-34559a866513\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>peopl</td>\n",
              "      <td>1879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trump</td>\n",
              "      <td>1855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>man</td>\n",
              "      <td>1832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>color</td>\n",
              "      <td>1822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>use</td>\n",
              "      <td>1787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>first</td>\n",
              "      <td>1707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>old</td>\n",
              "      <td>1621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>time</td>\n",
              "      <td>1582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>poster</td>\n",
              "      <td>1562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>day</td>\n",
              "      <td>1503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49f12eca-8e51-47c2-b9db-34559a866513')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49f12eca-8e51-47c2-b9db-34559a866513 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49f12eca-8e51-47c2-b9db-34559a866513');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list most uncommon words\n",
        "word_freq[-10:].reset_index(name=\"freq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5nBr7-6qKLUg",
        "outputId": "6dd08627-cdb5-42b2-b2c2-10baf40c349b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         index  freq\n",
              "0         eppa     1\n",
              "1      plotlin     1\n",
              "2       ceylon     1\n",
              "3      mandala     1\n",
              "4      walkman     1\n",
              "5        adden     1\n",
              "6      shermin     1\n",
              "7      marquez     1\n",
              "8      britian     1\n",
              "9  northkoreap     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87908f65-b5db-4fbc-9680-f5444ac58dc7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eppa</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>plotlin</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ceylon</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mandala</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>walkman</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>adden</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>shermin</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>marquez</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>britian</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>northkoreap</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87908f65-b5db-4fbc-9680-f5444ac58dc7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87908f65-b5db-4fbc-9680-f5444ac58dc7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87908f65-b5db-4fbc-9680-f5444ac58dc7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "show distribution of label column to check if data is balanced or not.\n"
      ],
      "metadata": {
        "id": "p-kL16S3pPAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of label\n",
        "train_data[\"label\"].value_counts(normalize=True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p368xFGzpFYT",
        "outputId": "b786e162-bc3f-42f2-e506-2a313e5e4720"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    0.575723\n",
              "1.0    0.424277\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into two columns"
      ],
      "metadata": {
        "id": "82V5fSgKVO5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splite train data into X,y columns\n",
        "X=train_data['clean_text'] \n",
        "y=train_data['label'] "
      ],
      "metadata": {
        "id": "ClA67-upKNGg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Tuning Vectorizer and classification models\n",
        "**TFidfVectorizer** \n",
        "\n",
        "\n",
        "**Vectorizer** used to convert the text data into numerical data\n",
        "\n",
        "\n",
        "**TF-IDF** is an abbreviation for Term Frequency Inverse Document Frequency. This is very common algorithm to transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction.[1]\n",
        "\n",
        "**max_df**  using  for removing terms that appear too frequently\n",
        "\n",
        "For example: max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\"\n",
        "\n",
        "**min_df** useding for removing terms that appear too infrequently.\n",
        "\n",
        "For example: min_df = 5 means \"ignore terms that appear in less than 5 documents\"\n",
        "\n",
        "**ngram_range** is just a string of n words in a row \n",
        "\n",
        "*    ngram_range=(1, 2) which mean use pairs of two words\n",
        "*    ngram_range=(1, 3) which mean use three words .\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r6MWWP77Vl9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline tuning \n",
        "# tune model's hyperparameter\n",
        "# tune Vectorizer \n",
        "# here I used (word-level vectorizer)\n",
        "\n",
        "pipeline = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\")), (\"lr_classifier\", LogisticRegression())])\n",
        "params = {    \n",
        "\"tfidf__ngram_range\": [(1, 2),(1,3)],                                   # tfidf__ngram_range points to tfidf->nngram_range\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.6),                                   # tfidf__max_df points to tfidf->max_df\n",
        "    \"tfidf__min_df\": np.arange(5,30 ),                                      # tfidf__min_df points to tfidf->min_df\n",
        "    'lr_classifier__penalty' : ['l1','l2'],                                 #lr_classifier__penalty points to lr_classifier->penalty\n",
        "    'lr_classifier__C' : np.logspace(-1,1,10),                              #lr_classifier__C points to lr_classifier->C\n",
        "    'lr_classifier__solver': [ 'liblinear','newton-cg', 'lbfgs']            #lr_classifier__solver points to lr_classifier->solver\n",
        "}"
      ],
      "metadata": {
        "id": "9ABgLO6FKQ9A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the experimental protocol used and how was it carried out?"
      ],
      "metadata": {
        "id": "GdxrewLgmJYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here I used validation set with (random , grid search ) \n",
        "\n",
        "Validation set is a set of data that is separate from the training set and is used to verify the performance of our model during training."
      ],
      "metadata": {
        "id": "BieZ0SvPma6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression model"
      ],
      "metadata": {
        "id": "rrgSBmphWBGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is very effective on text data and the underlying algorithm it also fairly easy to understand. More importantly, in the NLP world, it’s generally accepted that Logistic Regression is a great starter algorithm for text related classification. \n",
        "\n",
        "**Hyperparameter Tuning** \n",
        "\n",
        "Hyperparameters are an essential aspect of machine learning process. as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters  to give better results.\n",
        "\n",
        "So I tried to tune a list of parameters of Logisticregression's  hyper parameter such as **( penalty , C , solver )**\n",
        "\n",
        "**penalty** it's values is  ‘l1’, ‘l2’, ‘elasticnet’, ‘none’.\n",
        "\n",
        "Penalty is used to specify the norm used in the penalization \n",
        "\n",
        "The default value is ’l2’.\n",
        "\n",
        "**C** which mean inverse of regularization strength in Logistic Regression.and \n",
        " it be in range ( 0.001 , 0.01 , 0.1 , 1 , 10 )\n",
        "\n",
        "**solver** It provides options to choose solver algorithm for optimization.\n",
        "it's parameter 'liblinear','newton-cg', 'lbfgs"
      ],
      "metadata": {
        "id": "4WcLXpvCjy0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trial_1\n",
        "**Using Logistic regression model and grid search method (word-level vectorizer)**\n"
      ],
      "metadata": {
        "id": "cYZcrA5nWFIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **grid search**, we test every combination of values from a pre-defined list of hyper-parameters and pick the best one based on the cross validation score or validation set \n",
        "\n",
        "**Expectation**\n",
        "\n",
        "After cleaning and preprocessing the data using several techniques \n",
        "(removing the URL, removing all non-essential letters (Numbers and Punctuation ,convert all characters to lowercase, tokenization, removing stopwords, lemmatization ,remove the words with a length of 2) and using TfidfVectorizer (word-level)\n",
        "\n",
        "I tried grid search with Logistic regression classifier and set of hyperparameters tuned and vectorizer (word-level) to get a high performance\n",
        "\n",
        "my expectation that the accuraccy may be high\n",
        "\n",
        "**Observation** \n",
        "\n",
        "A fter running the model I found that the best hyperparameters that defined with grid search method are \n",
        "\n",
        "\n",
        "\n",
        "        best score 0.8730260693722905 ---> The model gave me\n",
        "        best hyperparameter {'lr_classifier__C': 2.1544346900318834, 'lr_classifier__penalty': 'l2', 'lr_classifier__solver': 'newton-cg', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 3)}\n",
        "\n",
        "        Score :  0.83985 on kaggle\n",
        "\n",
        "\n",
        "\n",
        "**plan** \n",
        "\n",
        "I will try to use the rondom search method with Logistic regression model and the same hyperparameters in trial_2\n"
      ],
      "metadata": {
        "id": "VJBy_9_eVJRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Further split the original training set to a train and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, train_size = 0.8, stratify = y, random_state = 42)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "#\n",
        "grid_search_lr = GridSearchCV(\n",
        "    pipeline, params, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search_lr.fit(X,y)\n",
        "\n",
        "print('best score {}'.format(grid_search_lr.best_score_))\n",
        "print('best hyperparameter {}'.format(grid_search_lr.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j--jR1Ojeq",
        "outputId": "992de2fe-061c-48cc-e82d-7524a7469844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 3000 candidates, totalling 3000 fits\n",
            "best score 0.8730260693722905\n",
            "best hyperparameter {'lr_classifier__C': 2.1544346900318834, 'lr_classifier__penalty': 'l2', 'lr_classifier__solver': 'newton-cg', 'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 3)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['label'] = grid_search_lr.predict_proba(test_data.clean_text)[:,1]\n",
        "\n",
        "submission.to_csv('trial1_Logistic_grid_word.csv', index=False)\n",
        "\n",
        "# Score: 0.83985 on kaggle"
      ],
      "metadata": {
        "id": "FL-3bCX0Ojgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trial_2\n",
        "**Using Logistic regression model and random search method (word-level vectorizer)**"
      ],
      "metadata": {
        "id": "5IF1f6L1WILt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random search** is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. and yet it has proven to yield better results comparatively.\n",
        "\n",
        "**Expectation**\n",
        "\n",
        "After cleaning and preprocessing the data using several techniques \n",
        "(removing the URL, removing all non-essential letters (Numbers and Punctuation ,convert all characters to lowercase, tokenization, removing stopwords, lemmatization ,remove the words with a length of 2) and using TfidfVectorizer (word-level)\n",
        "\n",
        "I tried random search with Logistic regression  classifier  and set of hyperparameters which must be tuned to get a high performance with Logistic regression model.\n",
        "\n",
        "My expectation that I may get a high accuraccy and The model run faster than using grid search \n",
        "\n",
        "\n",
        "**Observation** \n",
        "\n",
        "After running the model I found that the best hyperparameters that defined with random search method and TfidfVectorizer are\n",
        "\n",
        "        best score 0.8650467370071189\n",
        "        best hyperparameter {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 17, 'tfidf__max_df': 0.3, 'lr_classifier__solver': 'liblinear', 'lr_classifier__penalty': 'l2', 'lr_classifier__C': 0.774263682681127}\n",
        "\n",
        "        Score : 0.83719 on kaggle\n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will try to use the same search method with Logistic regression model but with using (character-level vectorizer) in trial_3"
      ],
      "metadata": {
        "id": "fylUYLs9VQZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Further split the original training set to a train and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, train_size = 0.8, stratify = y, random_state = 42)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "#\n",
        "random_search_lr = RandomizedSearchCV(\n",
        "    pipeline, params, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "random_search_lr.fit(X,y)\n",
        "\n",
        "print('best score {}'.format(random_search_lr.best_score_))\n",
        "print('best hyperparameter {}'.format(random_search_lr.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0DPTcoOKX5g",
        "outputId": "b674bc3d-2083-4e22-b0f7-8491d23da9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
            "best score 0.8650467370071189\n",
            "best hyperparameter {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 17, 'tfidf__max_df': 0.3, 'lr_classifier__solver': 'liblinear', 'lr_classifier__penalty': 'l2', 'lr_classifier__C': 0.774263682681127}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['label'] = random_search_lr.predict_proba(test_data.clean_text)[:,1]\n",
        "\n",
        "submission.to_csv('trial2_Logistic_random_word.csv', index=False)\n",
        "\n",
        "# Score: 0.83719 on kaggle"
      ],
      "metadata": {
        "id": "YWtb2hsnKjiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trial_3\n",
        "**Using Logistic regression model and random search method (character-level vectorizer)**"
      ],
      "metadata": {
        "id": "bX2yXQIdWdJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation**\n",
        "\n",
        "After cleaning and preprocessing the data using several techniques (removing the URL, removing all non-essential letters (Numbers and Punctuation ,convert all characters to lowercase, tokenization, removing stopwords, lemmatization ,remove the words with a length of 2) and using TfidfVectorizer (character-level)\n",
        "\n",
        "I tried random search with Logistic regression classifier model and set of hyperparameters which must be tune to get a high performance with Logistic regression model.\n",
        "\n",
        "my expectation that the accuracy will not be higher than using the Vectorizer with ( character level )\n",
        "\n",
        "**Observation**\n",
        "\n",
        "After running the model I found that the accuracy is lower than using Vectorizer (character-level) and the best hyperparameters that defined with rondom search method and TfidfVectorizer are\n",
        "\n",
        "        best score 0.8398320275772793\n",
        "        best hyperparameter {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 7, 'tfidf__max_df': 0.3, 'lr_classifier__solver': 'newton-cg', 'lr_classifier__penalty': 'l2', 'lr_classifier__C': 3.593813663804626}\n",
        "\n",
        "        Score : 0.0.77987 0n kaggle\n",
        "when using analyzer= character, I found the model accuracy became lower than using analyzer = word \n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will try to use the grid search method with XGBoost model (word-level vectorizer) on trial_4"
      ],
      "metadata": {
        "id": "gkjP855uY-En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline tuning \n",
        "# tune model's hyperparameter\n",
        "# tune Vectorizer \n",
        "# here I used (char-level vectorizer)\n",
        "\n",
        "pipeline = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\")), (\"lr_classifier\", LogisticRegression())]) \n",
        "\n",
        "params = {    \n",
        "    \"tfidf__ngram_range\": [(1, 2),(1,3)],                                   # tfidf__ngram_range points to tfidf->nngram_range\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.6),                                   # tfidf__max_df points to tfidf->max_df\n",
        "    \"tfidf__min_df\": np.arange(5,30 ),                                      # tfidf__min_df points to tfidf->min_df\n",
        "    'lr_classifier__penalty' : ['l1','l2'],                                 #lr_classifier__penalty points to lr_classifier->penalty\n",
        "    'lr_classifier__C' : np.logspace(-1,1,10),                              #lr_classifier__C points to lr_classifier->C\n",
        "    'lr_classifier__solver': [ 'liblinear','newton-cg', 'lbfgs']            #lr_classifier__solver points to lr_classifier->solver\n",
        "}\n"
      ],
      "metadata": {
        "id": "guh--W5dNy5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Further split the original training set to a train and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, stratify = y, random_state = 40)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "random_search_lr_char = RandomizedSearchCV(\n",
        "    pipeline, params, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc')\n",
        "    # number of random trials\n",
        "    \n",
        "\n",
        "random_search_lr_char.fit(X,y)\n",
        "\n",
        "print('best score {}'.format(random_search_lr_char.best_score_))\n",
        "print('best hyperparameter {}'.format(random_search_lr_char.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtYPuY3bKuxQ",
        "outputId": "7f502ab7-aa47-4c7e-db5b-1cfe9d6add0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
            "best score 0.8398320275772793\n",
            "best hyperparameter {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 7, 'tfidf__max_df': 0.3, 'lr_classifier__solver': 'newton-cg', 'lr_classifier__penalty': 'l2', 'lr_classifier__C': 3.593813663804626}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['label'] = random_search_lr_char.predict_proba(test_data.clean_text)[:,1]\n",
        "\n",
        "submission.to_csv('trial3_Logistic_random_char.csv', index=False)\n",
        "\n",
        "# best score 0.8221684598839467\n",
        "# best hyperparameter {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 15, 'tfidf__max_df': 0.3, 'lr_classifier__solver': 'newton-cg', 'lr_classifier__penalty': 'l2', 'lr_classifier__C': 0.2782559402207124}"
      ],
      "metadata": {
        "id": "foeGpGHcLQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost model"
      ],
      "metadata": {
        "id": "7NANJPrdA_jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "XGBoost machine learning models provide the best combination of prediction performance and processing time when compared to other algorithms, It's also a library for creating gradient boosting tree models that are both quick and high-performing. That XGBoost outperforms the competition on a variety of tough machine learning tasks becuse it is  powerful enough to deal with all sorts of irregularities of data. So I will use this model to try to get a highly prediction.\n",
        "\n",
        "\n",
        "**To improve and get fully use the benefits  of  XGBoost model,  hyper  parameter tuning is must.**\n",
        "\n",
        "And It is very difficult to get answers to practical questions like\n",
        "\n",
        "*   Which set of parameters you should tune ?\n",
        "*   What is the ideal value of these parameters to obtain optimal output ? \n",
        "\n",
        "So I tried to tune a list of parameters of XGBoost hyper parameter such as   **(learning_rate, max_depth , gamma ,n_estimators)**\n",
        "\n",
        "**learning_rate :**Step size shrinkage used in update to prevents overfitting.[3]\n",
        "\n",
        "it's value must be between 0 and 1 to optimizes the chances to reach the best optimum\n",
        "\n",
        "I choosed a list that has values = [0.01,0.05,0.1] \n",
        "\n",
        "that it lay on the range from 0.01 to 0.3 \n",
        "\n",
        "and it's Default = 0.3. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**max_depth:** which mean the maximum depth of a tree we use it.\n",
        "\n",
        "if I use a high max_depth, performance  might increase but the model's complexity will increase and lead to overfit on the other hand if I choose a very small max_depth if made my model can't able to learn well and lead to underfit \n",
        "so I choosed a list of max_depth paramters to get the most suitable max_depth to make our model increase it's performance \n",
        "\n",
        "I choosed a list that has values = [60,70,75]\n",
        "\n",
        "**gamma:** which mean  is a pseudo-regularisation parameter (Lagrangian multiplier), and depends on the other parameters. The higher Gamma is, the higher the regularization. It can be any integer. Default is 0 [4]\n",
        "\n",
        "I choosed a list that has values = [ 0.1, 0.2 , 0.3]\n",
        "\n",
        "\n",
        "**n_estimators:** which mean  number of trees in our ensemble.\n",
        "\n",
        "why we use this hyper parameter in our model?!!!\n",
        "\n",
        "The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees. [5]\n",
        "\n",
        "I choosed a list that has values = [150,200,250] \n",
        "\n",
        "it's Default = 100.\n"
      ],
      "metadata": {
        "id": "NBJv5Ym7Z5B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline tuning \n",
        "# tune model's hyperparameter\n",
        "# tune Vectorizer \n",
        "# here I used (word-level vectorizer)\n",
        "\n",
        "pipeline = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\")), (\"xgb_classifier\",XGBClassifier())])\n",
        "\n",
        "params = {    \n",
        "    \"tfidf__ngram_range\": [(1, 2),(1,3)],                    # tfidf__ngram_range points to tfidf->nngram_range\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.5),                    # tfidf__max_df points to tfidf->max_df\n",
        "    \"tfidf__min_df\": np.arange(5,30),                        # tfidf__min_df points to tfidf->min_df\n",
        "    'xgb_classifier__learning_rate' : [0.01,0.05,0.1],       # my_classifier__n_estimators points to my_classifier->learning_rate\n",
        "    'xgb_classifier__max_depth' : [60,70,75],                # my_classifier__n_estimators points to my_classifier->max_depth \n",
        "    'xgb_classifier__gamma': [ 0.1, 0.2 , 0.3],              # my_classifier__n_estimators points to my_classifier->gama\n",
        "    'xgb_classifier__n_estimators': [150,200,250]            # my_classifier__n_estimators points to my_classifier->gamma\n",
        "}\n"
      ],
      "metadata": {
        "id": "5PMvkdlVCMP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial_4 \n",
        "**Using XGBoost model and random search method (word-level vectorizer)**"
      ],
      "metadata": {
        "id": "HPm1QLTJC6zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation**\n",
        "\n",
        "After cleaning and preprocessing the data using several techniques \n",
        "(removing the URL, removing all non-essential letters (Numbers and Punctuation ,convert all characters to lowercase, tokenization, removing stopwords, lemmatization ,remove the words with a length of 2) and using TfidfVectorizer (word-level)\n",
        "\n",
        "I tried random search with XGBoost classifier model and set of hyperparameters which must be tune to get a high performance with XGBoost model.\n",
        "\n",
        "my expectation that I may get a high accuraccy and get the best hyperparamter which the model need to get high performance\n",
        "\n",
        "**Observation**\n",
        "\n",
        "After running the model I found that the best hyperparameters that defined with rondom search method are\n",
        "\n",
        "\n",
        "        best score 0.8563903704660246\n",
        "        best hyperparameter {'xgb_classifier__n_estimators': 250, 'xgb_classifier__max_depth': 60, 'xgb_classifier__learning_rate': 0.1, 'xgb_classifier__gamma': 0.1, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 15, 'tfidf__max_df': 0.3}\n",
        "        Score: 0.80346 on kaggle\n",
        "\n",
        "**Plan**\n",
        "\n",
        "I will try to use grid search method with  RandomForest model and  analyzer = character  "
      ],
      "metadata": {
        "id": "sI0N3ocljbNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Further split the original training set to a train and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, stratify = y, random_state = 40)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "random_xgb_word = RandomizedSearchCV(\n",
        "    pipeline, params, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    n_iter=10,\n",
        "    scoring='roc_auc')\n",
        "    # number of random trials\n",
        "    \n",
        "\n",
        "random_xgb_word.fit(X,y)\n",
        "\n",
        "print('best score {}'.format(random_xgb_word.best_score_))\n",
        "print('best hyperparameter {}'.format(random_xgb_word.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0c76LMGCz20",
        "outputId": "45c5bd49-f6ba-436f-b9f6-7bf6e14f97e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
            "best score 0.8551628296279068\n",
            "best hyperparameter {'xgb_classifier__n_estimators': 250, 'xgb_classifier__max_depth': 60, 'xgb_classifier__learning_rate': 0.1, 'xgb_classifier__gamma': 0.3, 'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 19, 'tfidf__max_df': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['label'] = random_xgb_word.predict_proba(test_data.clean_text)[:,1]\n",
        "\n",
        "submission.to_csv('trial4_xgb_random_word.csv', index=False)"
      ],
      "metadata": {
        "id": "BV3EOfLRh38u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForest model"
      ],
      "metadata": {
        "id": "QMSrBgO8jf_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trial_5\n",
        "**Using RandomForest model and grid search method (character-level vectorizer)**"
      ],
      "metadata": {
        "id": "LFiHuCDoiedG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classifier** is a classification algorithm made up of several decision trees. The algorithm uses randomness to build each individual tree to promote uncorrelated forests, which then uses the forest's predictive powers to make accurate decisions.\n",
        "\n",
        "hyperparameters in random forest are important  to increase the predictive power of the model or to make the model faster.\n",
        "\n",
        "**n_estimators**  hyperparameter, which is just the number of trees the algorithm builds before taking the maximum voting or taking the averages of predictions. In general, a higher number of trees increases the performance and makes the predictions more stable, but it also slows down the computation. [7]"
      ],
      "metadata": {
        "id": "SxcB6UTIgwoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "pipeline = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\")), (\"randonforest_classifier\",RandomForestClassifier())])\n",
        "\n",
        "params = {\"tfidf__ngram_range\": [(1, 2),(1,3)],                    # tfidf__ngram_range points to tfidf->nngram_range\n",
        "          \"tfidf__max_df\": np.arange(0.3, 0.5),                    # tfidf__max_df points to tfidf->max_df\n",
        "          \"tfidf__min_df\": np.arange(5,15),                        # tfidf__min_df points to tfidf->min_df\n",
        "          'randonforest_classifier__n_estimators': [250]           # my_classifier__n_estimators points to my_classifier->n_estimators\n",
        "}"
      ],
      "metadata": {
        "id": "ZtIU0y-xfxLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation**\n",
        "\n",
        "After cleaning and preprocessing the data using several techniques (removing the URL, removing all non-essential letters (Numbers and Punctuation ,convert all characters to lowercase, tokenization, removing stopwords, lemmatization ,remove the words with a length of 2) and using TfidfVectorizer (word-level)\n",
        "\n",
        "I tried grid search with Randomforest classifier model and it's hyperparameter (n_estimators)  which must be tune to get a high performance with Randomforest model.\n",
        "\n",
        "my expectation that I will get a high accuraccy and get the best hyperparamter which the model need to get high performance\n",
        "\n",
        "**Observation**\n",
        "\n",
        "After running the model I found that the best hyperparameters that defined with rondom search method are\n",
        "\n",
        "      best score 0.8151476406921723\n",
        "      best hyperparameter {'randonforest_classifier__n_estimators': 250, 'tfidf__max_df': 0.3, 'tfidf__min_df': 9, 'tfidf__ngram_range': (1, 3)}\n",
        " \n",
        "      Score: 0.84910 on kaggle"
      ],
      "metadata": {
        "id": "eMhSqTVyiDqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Further split the original training set to a train and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, train_size = 0.8, stratify = y, random_state = 42)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_train2 (new training set), X_train\n",
        "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "#\n",
        "grid_search_random = GridSearchCV(\n",
        "    pipeline, params, cv=pds, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    scoring='roc_auc')\n",
        "\n",
        "grid_search_random.fit(X,y)\n",
        "\n",
        "print('best score {}'.format(grid_search_random.best_score_))\n",
        "print('best hyperparameter {}'.format(grid_search_random.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0for9uSinsu",
        "outputId": "d2db89e6-e06f-487f-c94c-4dc0a5b70d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 4050 candidates, totalling 4050 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = test_data.index\n",
        "\n",
        "submission['label'] = grid_search_random.predict_proba(test_data.clean_text)[:,1]\n",
        "\n",
        "submission.to_csv('trial5_randomforest_char.csv', index=False)"
      ],
      "metadata": {
        "id": "lk39QCEQiwFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Formulation"
      ],
      "metadata": {
        "id": "H9t43e1VEZD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the problem** \n",
        "\n",
        "Because of the increase of social networks and their involvement in several spheres such as politics, false information on the Internet has generated many social difficulties. So we need to predict if a specific reddit post is fake news or not, by looking at its title. And the provided data is raw (contains various forms of words) wherefore we need to apply text preprocessing techniques to clean it and make a good prediction. \n",
        "\n",
        "**What is the input?**\n",
        "\n",
        "The input is text data which include the tile of news that we want to predict if the given post is fake news or not .\n",
        "\n",
        "**What is the output?**\n",
        "\n",
        "The output is a prediction if a specific reddit post is fake news or not, by looking at its title.\n",
        "\n",
        "**What data mining function is required?** \n",
        "\n",
        "The data mining function is binary classification\n",
        "\n",
        "**What could be the challenges?**\n",
        "The challenges are that :\n",
        "\n",
        "The data is raw (contains various forms of words)  and need text preprocessing techniques to be applied on it. \n",
        "\n",
        "We need to implement a model to predict whether the post is fake or not and search for the hyperparameters to get the higher performance of our model.\n",
        "\n",
        "**What is the impact?** \n",
        "\n",
        "The impact is that we want to implement a model that can predict if the post is fake news or not  based on the title of news which mar has no impact \n",
        "\n",
        "\n",
        "**What is an ideal solution?** \n",
        "\n",
        "The best accuracy I got when using Randomforest model with grid search method"
      ],
      "metadata": {
        "id": "Y9kTgJ8jEjCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Questions :"
      ],
      "metadata": {
        "id": "TOPNSrk5W8yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?**\n",
        "\n",
        "**character-level n-grams** divides a text into a collection of characters and require far less storage space and, as a result, will carry much less data\n",
        "\n",
        "**word-level n-grams** devide a sentence into a collection of words and  may serve the same purposes and much more, but they need much more storage.\n",
        "\n",
        "Because of the new terms that exist in the testing dataset but do not appear in the training dataset, word n-grams are more prone to OOV (Out-Of-Vocabulary) problems.\n",
        "\n",
        "OOV words are handled logically by Character Tokenizers by maintaining the word's information. It decomposes the OOV word into characters and expresses it using these characters.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**2- What is the difference between stop word removal and stemming? Are these techniques language-dependent?**\n",
        "\n",
        "**Stop word removal :** stopwords are the most common words in a text that provide no useful information. Stopwords such as they, there, this, and where are examples of stopwords.\n",
        "\n",
        "**Stemming :** stemming is the process of deleting a component of a word or reducing a word to its stem or root \n",
        "\n",
        "Yes, these techniques considered language dependent \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**3-  Is tokenization techniques language dependent? Why?**\n",
        "\n",
        "Yes, tokenization techniques is highly language dependent because  \n",
        "\n",
        "In natural language processing, programming languages work by breaking down raw code into tokens and then combining them using logic (the program's grammar).\n",
        "\n",
        "We can apply a modest set of principles to integrate the text into some bigger meaning by breaking it up into little, known fragments.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**4- What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?**\n",
        "\n",
        "TfidfVectorizer and CountVectorizer are both methods for turning text input into vectors.because the model can only handle numerical data but  in CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. \n",
        "\n",
        "TF-IDF is preferable to Count Vectorizers in that it not only considers the frequency of words found in the corpus, but also their importance.\n",
        "\n",
        "No,  It is not feasible to use all possible n-grams because it depends on the application, so is not possible to use all potential n-grams. Instead, we should experiment with different n-grams on your data to see which one performs best in the mod"
      ],
      "metadata": {
        "id": "0pIO-HRbXU_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refrences\n"
      ],
      "metadata": {
        "id": "HfKxkMy1Vrf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a\n",
        "\n",
        "[2] https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
        "\n",
        "[3] https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "\n",
        "[4] https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
        "\n",
        "[5] https://analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/#:~:text=Random%20search%20is%20a%20technique,yields%20high%20variance%20during%20computing.\n",
        "\n",
        "[6] https://medium.com/analytics-vidhya/text-preprocessing-for-nlp-natural-language-processing-beginners-to-master-fd82dfecf95\n",
        "\n",
        "[7] https://builtin.com/data-science/random-forest-algorithm"
      ],
      "metadata": {
        "id": "ZfbCSKl3Qhtd"
      }
    }
  ]
}